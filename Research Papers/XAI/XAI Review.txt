XAI Papers

1. A comparative analysis of rule-based,
model-agnostic methods for explainable articial
intelligence

Abstract: High accuracy + explainability: a process that discloses the relationship between input and output. As title suggests this is a 
comparison on methods for extracting inference rules not explicit within a model.

Conclusion: This study presented a novel approach to evaluate and compare four XAI meth-
ods which extract rules from black-box machine-learned models, trained via
vanilla neural networks on eight datasets.

One of the methods, specically
the C45-Pane, based on a feature splitting algorithm, outperformed the others.

Suggestion: Worth reading in more detail.



2. A Quantitative Evaluation of Global,
Rule-Based Explanations of Post-Hoc,
Model Agnostic Methods

Abstract + Intro: Similar to Paper 1 - discussion on inference rules and how to make explcit to end users. Again, like Paper 1 a comparison 
but also a discussion on combining the methods.

Good description at start of Intro of 'black-box' perception - does tally well with my proposal. also describes the creation of 'if-then' 
rules that are understanable by humans. This study proposes a method to look at metrics to compare explainability of different if-then 
rule extraction methods


Conclusion: Much the same as Paper 1 - can be cited, therfore superior. In hindsight, this is a better paper.

Goes into details on metrics that can be used for ranking 'if-then' rules.

Suggestion: Read over quickly in companion with Paper 1 - concepts are interesting and aligned with my proposal.



3. Classification of Explainable Artificial Intelligence Methods
through Their Output Formats

Abstract: Machine and deep learning have proven their utility to generate data-driven models with
high accuracy and precision. However, their non-linear, complex structures are often difficult to
interpret. Consequently, many scholars have developed a plethora of methods to explain their
functioning and the logic of their inferences. Use Output methods to put these methods into a structure.

Help select an approach based on the problem. The idea is to guide in the selection of an XAI approach for a given problem.

Intro: Again with the 'black-box' description...and need for humans to understand reason for decision. The use of the output
format in XAI method selection seems to have been a research gap in the past.

AI practitioners find useful explanations that accurately
reflect the logic implemented within a model. In this case, rule-based explanations represent
a structured, compact yet comprehensible way to represent a set of logical instructions.
On the other hand, end-users belonging to the lay public prefer reconstructive explanations
that build a ‘story’, exposing which input features contribute the most to the model’s
prediction.


See Page 12 about rules-based explanations for NN, usually in the form of IF-THEN rules


Conclusion: The term XAI groups together the scientific body of knowledge developed while
searching for methods to explain the inner logic of either a learning algorithm, a model
induced from it, or a knowledge-based approach for inference and it is now generally recognised
as a core area of AI. (This might be a useful quote).

There is a lack of consensus around what explainability actually is.

Proposing an XAI framework with a universal outlook. the five
dimensions of the XAI methods discussed in this literature review, namely the stage, scope,
problem, input data and format of the explanation. Nice picture at the end.


Suggestion: Definitely read. Try and find a recommendation in the part for CC Fraud; this could be a strong citation. 
Find 'domain-specific' measure for me.




4. Explainable Artificial Intelligence:
Concepts, Applications, Research
Challenges and Visions

Abstract: The goal of this
research is to articulate the big picture ideas and their role in advancing
the development of XAI systems, to acknowledge their historical roots,
and to emphasise the biggest challenges to moving forward.

Mentions GDPR AI regulatory concerns - may have relevance for CC Fraud.

Intro: Rework of 'black-box' descriptions. On
the one hand, a large body of work have focused on building post-hoc methods
mainly aimed at wrapping fully trained models, often referred to black-boxes,
with an explainability layer [37]. A smaller body of research works, on the other
hand, have concentrated on creating self-explainable and interpretable models
by incorporating explainability mechanisms during their training, often referred
to as the ante-hoc phase [7].

Conclusion: Rewokr of the above.

Suggestion: Looks like more of a historical review - skim.





5. A Global Model-Agnostic XAI method for the
Automatic Formation of an Abstract Argumentation
Framework and its Objective Evaluation

Abstract: This research study explored a new way for the automatic
formation of an argument-based representation of the inference process of a data-driven ML model to
enhance its explainability by employing principles and techniques from computational argumentation

Conclusion: The C4.5 algorithm usually
generate bigger rulesets, but it is more correct and faithful to the model than the argument-based
method. In conclusion, the proposed XAI method returns rulesets that are complete, simpler and
smaller in terms of rule cardinality and length, thus more comprehensible. However, they are not
as faithful to the model, correct and robust as the C4.5 DTs.

Suggestion: Probably too specific in aim right now - skip for CA1





6. Notions of explainability and evaluation approaches for explainable artificial
intelligence

Abstract: XAI approaches can be clustered into human-centred evaluations and evaluations
with more objective metrics. However, despite the vast body of knowledge developed around the concept of
explainability, there is not a general consensus among scholars on how an explanation should be defined, and
how its validity and reliability assessed. Eventually, this review concludes by critically discussing these gaps
and limitations, and it defines future research directions with explainability as the starting component of any
artificial intelligent system.

Covers much the same group as Paper 1 + 2; Intro re-does the whole 'Black-Box' thing.

USEFUL Quote; "Article 22 of the General Data Protection Regulation (GDPR) sets out the rights
and obligations of the use of automated decision making. Noticeably,
it introduces the right of explanation by giving individuals the right to
obtain an explanation of the inference(s) automatically produced by a
model, confront and challenge an associated recommendation, particularly
when it might negatively affect an individual legally, financially,
mentally or physically. By approving this GDPR article, the European
Parliament attempted to tackle the problem related to the propagation
of potentially biased inferences to society, that a computational model
might have learnt from biased and unbalanced data."


Conclusion: This research effort has produced many
definitions of explainability and identified numerous notions related
to it, such as interpretability, understandability, comprehensibility and
justifiability, just to mention a few. Coupled to these notions, different
qualitative and quantitative measures have also been coined for
their assessment.


See diagram on 'explanators' + 'evaluation'.


Note - avoid getting into phychology aspect of explainability, as decibed in Conclusion, don't want to go down that route.

Suggestion: Skim to see if any particularly useful CC Fraud stuff emerges.









7. A Novel Human-Centred Evaluation
Approach and an Argument-Based
Method for Explainable Artificial
Intelligence

Abstract: Similar to Paper 5 but describes a huma based experiment comparing argument-based representation to a DT one
with two sub-groups.

NOT an approach I want to follow.


Intro; Descrines 'blac;-box' concept again, and then how augumentation is a viable XAI method

Conclusion: This study proposed a novel XAI method to extract IF-THEN rules automatically
from ML models and treat them as arguments in the form of premises
to a conclusion.

Suggestion: Human experimentation might be interesting, but pass on this  one for CA1


Reading Order - 3,1,2,6,4





7a - A Unified Approach to Interpreting Model
Predictions


Abstract: A tension between accuracy and interpretability. Presenting a Unified framework for interpreting predictions - SHAP.
SHAP assigns each feature an importance value for a particular prediction.

The new class unifies six existing methods, notable becauseseveral recent methods in the class lack the proposed desirable properties.
(Come back to this...)

Intro: We propose new SHAP value estimation methods and demonstrate that they are better aligned
with human intuition as measured by user studies and more effectually discriminate among model
output classes than several existing methods


Conclusion: We presented several different estimation methods for SHAP values, along with proofs and experiments
showing that these values are desirable.


Suggestion: Need to consider in context with other material, such as stuff on LIME.






4 -  Peeking Inside the Black-Box: A Survey on
Explainable Artificial Intelligence (XAI)

Abstract: General Black-box idea - review of existing approaches.

Intro: AI algorithms suffer from opacity,
that it is difcult to get insight into their internal mechanism
of work, especially Machine Learning (ML) algorithms.
Which further compound the problem, because entrusting
important decisions to a system that cannot explain itself
presents obvious dangers.


Technically, there is no standard and generally accepted
denition of explainable AI.

Even though a multitude of techniques is used in literature
to enable global interpretability. Arguably, global model
interpretability is hard to achieve in practice, especially for
models that exceed a handful of parameters. Analogically to
human, who focus effort on only part of the model in order
to comprehend the whole of it, local interpretability can be
more readily applicable.

Another observation that should be noted is that in the
reviewed literature, local explanations is the most used methods
to generate explanations in DNNs.



Conclusion: We presented in
this paper a comprehensive background regarding this eld.


Suggestion: Interesting but hard to generalise.





5 - Explainable Machine
Learning for Fraud
Detection

Abstract: We explore explainability methods in the
domain of real-time fraud detection by investigating the
selection of appropriate background data sets and runtime
tradeoffs on supervised and unsupervised models.


Intro: Re-run of black-box but also the need to met GDPR demands. 
In this article, we present a case study
that explores explanations with two
of the most prominent methods, LIME
and SHAP, to explain fraud detected by
both supervised and unsupervised models.

Attribution techniques explain a
single-instance prediction by ranking the
most important features that affected
the generation of it.


LIME17 approximates the predictions
of the underlying black-box model
by training local surrogate models to
explain individual predictions. Essentially,
LIME modifies a single data sample
by tweaking the feature values in the
simpler local model and observes the
resulting impact on the output.


The SHAP18 method explains the
prediction of an instance by computing
the contribution of each feature to the
prediction using Shapley values based
on coalition game theory. Intuitively,
SHAP quantifies the importance of
each feature by considering the effect
each possible feature combination has
on the output.



Conclusion: For advanced machine learning
algorithms to be successfully
adopted in the financial domain,
model explainability is necessary to
address regulatory requirements and
ensure trust in the results.

We also found that, while SHAP
gives more reliable explanations, LIME
is faster. In real-time systems, it is not
always feasible to explain everything.
We must balance the deployability of the
models and explanation methods with
the time needed for a human and the
likelihood of fraud. It may be beneficial
to use a combination of both methods,
where LIME is utilized to provide realtime
explanations for fraud prevention,
and SHAP is used to enable regulatory
compliance and examine the model
accuracy in retrospect





Look up - 

Methods for Fraud Detection:
A. Thennakoon, C. Bhagyani, S.
Premadasa, S. Mihiranga, and
N. Kuruwitaarachchi, “Realtime
credit card fraud detection
using machine learning,” in Proc.
2019 9th Int. Conf. Cloud Comput.,
Data Science Eng. (Confluence),
pp. 488–493. doi: 10.1109/


LIME:
M. T. Ribeiro, S. Singh, and C.
Guestrin, “why should I trust
you?” explaining the predictions
of any classifier,” in Proc. 22nd
ACM SIGKDD Int. Conf. Knowledge
Discovery Data Mining, 2016, pp.
1135–1144.


SHAP: Have that one already - Lundberg + Lee




8a - “Why Should I Trust You?”
Explaining the Predictions of Any Classifier


Abstract: In this work, we propose LIME, a novel explanation tech-
nique that explains the predictions of any classier in an in-
terpretable and faithful manner, by learning an interpretable
model locally around the prediction.

Intro: Rework of Abstract.

Conclusions: Our
experiments demonstrated that explanations are useful for a
variety of models in trust-related tasks in the text and image
domains, with both expert and non-expert users: deciding
between models, assessing trust, improving untrustworthy
models, and getting insights into predictions.





























